if (sum(abs(beta_new - beta_old)) < tol) break;
}
beta_mat[, l] = beta_new
}
return(beta_mat)
}
# fit the model
myfit = myLasso_pw(X, y, lambda_all)
par(mfrow = c(1, 2))
matplot(t(myfit[1:10, ]), type = "l", xlab = "Lambda Index",
ylab = "Estimated Beta", main = "My Lasso")
matplot(t(glmnetfit$beta[1:10, ]), type = "l", xlab = "Lambda Index",
ylab = "Estimated Beta", main = "Glmnet")
library(glmnet)
glmnetfit = glmnet(X, y, intercept = FALSE)
lambda_all = glmnetfit$lambda
myLasso_pw <- function(X, y, all_lambda, tol = 1e-7, maxitr = 100){
n = nrow(X)
p = ncol(X)
nlambda = length(all_lambda)
all_lambda = sort(all_lambda, decreasing = TRUE)
beta_mat = matrix(NA, p, nlambda)
beta_old = rep(0, p)
beta_new = beta_old
for (l in 1:nlambda){
lambda = all_lambda[l]
# the grand loop
for (k in 1:maxitr){
beta_old = beta_new
r = y - X %*% beta_old
# update each beta_j in a loop
for (j in 1:p){
# first, we calculate the one-dimensional regression beta paramter by removing the effect of others
r = r + X[, j] * beta_new[j]
# then, we apply the soft threshold
beta_new[j] <- soft_th( X[, j] %*% r / n, lambda)
# update r to reflect the change of beta_j
r = r - X[, j] * beta_new[j]
}
# check if the loss function change is small
if (sum(abs(beta_new - beta_old)) < tol) break;
}
beta_mat[, l] = beta_new
}
return(beta_mat)
}
# fit the model
myfit = myLasso_pw(X, y, lambda_all)
par(mfrow = c(1, 2))
matplot(t(myfit[1:10, ]), type = "l", xlab = "Lambda Index",
ylab = "Estimated Beta", main = "My Lasso")
matplot(t(glmnetfit$beta[1:10, ]), type = "l", xlab = "Lambda Index",
ylab = "Estimated Beta", main = "Glmnet")
library(glmnet)
glmnetfit = glmnet(X, y, intercept = FALSE)
lambda_all = glmnetfit$lambda
myLasso_pw <- function(X, y, all_lambda, tol = 1e-7, maxitr = 100){
n = nrow(X)
p = ncol(X)
size_lambda = length(all_lambda)
all_lambda = sort(all_lambda, decreasing = TRUE)
beta_mat = matrix(NA, p, size_lambda)
beta_old = rep(0, p)
beta_new = beta_old
for (l in 1:size_lambda){
lambda = all_lambda[l]
# the grand loop
for (k in 1:maxitr){
beta_old = beta_new
r = y - X %*% beta_old
for (j in 1:p){
r = r + X[, j] * beta_new[j]
# then, we apply the soft threshold
beta_new[j] <- soft_th( X[, j] %*% r / n, lambda)
r = r - X[, j] * beta_new[j]
}
if (sum(abs(beta_new - beta_old)) < tol) break;
}
beta_mat[, l] = beta_new
}
return(beta_mat)
}
# fit myLasso models
myfit = myLasso_pw(X, y, lambda_all)
#Plot the models
par(mfrow = c(1, 2))
matplot(t(myfit[1:10, ]), type = "l", xlab = "Lambda Index",
ylab = "Estimated Beta", main = "My Lasso")
matplot(t(glmnetfit$beta[1:10, ]), type = "l", xlab = "Lambda Index",
ylab = "Estimated Beta", main = "Glmnet")
library(glmnet)
glmnetfit = glmnet(X, y, intercept = FALSE)
lambda_all = glmnetfit$lambda
myLasso_pw <- function(X, y, all_lambda, tol = 1e-7, maxitr = 100){
n = nrow(X)
p = ncol(X)
size_lambda = length(all_lambda)
all_lambda = sort(all_lambda, decreasing = TRUE)
beta_mat = matrix(NA, p, size_lambda)
fist_beta = rep(0, p)
beta_new = fist_beta
for (l in 1:size_lambda){
lambda = all_lambda[l]
# the grand loop
for (k in 1:maxitr){
fist_beta = beta_new
r = y - X %*% fist_beta
for (j in 1:p){
r = r + X[, j] * beta_new[j]
# then, we apply the soft threshold
beta_new[j] <- soft_th( X[, j] %*% r / n, lambda)
r = r - X[, j] * beta_new[j]
}
if (sum(abs(beta_new - fist_beta)) < tol) break;
}
beta_mat[, l] = beta_new
}
return(beta_mat)
}
# fit myLasso models
myfit = myLasso_pw(X, y, all_lambda)
library(glmnet)
glmnetfit = glmnet(X, y, intercept = FALSE)
lambda_all = glmnetfit$lambda
myLasso_pw <- function(X, y, all_lambda, tol = 1e-7, maxitr = 100){
n = nrow(X)
p = ncol(X)
size_lambda = length(all_lambda)
all_lambda = sort(all_lambda, decreasing = TRUE)
beta_mat = matrix(NA, p, size_lambda)
fist_beta = rep(0, p)
beta_new = fist_beta
for (l in 1:size_lambda){
lambda = all_lambda[l]
# the grand loop
for (k in 1:maxitr){
fist_beta = beta_new
r = y - X %*% fist_beta
for (j in 1:p){
r = r + X[, j] * beta_new[j]
# then, we apply the soft threshold
beta_new[j] <- soft_th( X[, j] %*% r / n, lambda)
r = r - X[, j] * beta_new[j]
}
if (sum(abs(beta_new - fist_beta)) < tol) break;
}
beta_mat[, l] = beta_new
}
return(beta_mat)
}
# fit myLasso models
myfit = myLasso_pw(X, y, all_lambda)
library(glmnet)
glmnetfit = glmnet(X, y, intercept = FALSE)
lambda_all = glmnetfit$lambda
myLasso_pw <- function(X, y, all_lambda, tol = 1e-7, maxitr = 100){
n = nrow(X)
p = ncol(X)
size_lambda = length(all_lambda)
all_lambda = sort(all_lambda, decreasing = TRUE)
beta_mat = matrix(NA, p, size_lambda)
fist_beta = rep(0, p)
beta_new = fist_beta
for (l in 1:size_lambda){
lambda = all_lambda[l]
# the grand loop
for (k in 1:maxitr){
fist_beta = beta_new
r = y - X %*% fist_beta
for (j in 1:p){
r = r + X[, j] * beta_new[j]
# then, we apply the soft threshold
beta_new[j] <- soft_th( X[, j] %*% r / n, lambda)
r = r - X[, j] * beta_new[j]
}
if (sum(abs(beta_new - fist_beta)) < tol) break;
}
beta_mat[, l] = beta_new
}
return(beta_mat)
}
# fit myLasso models
myfit = myLasso_pw(X, y, lambda_all)
#Plot the models
par(mfrow = c(1, 2))
matplot(t(myfit[1:10, ]), type = "l", xlab = "Lambda Index",
ylab = "Estimated Beta", main = "My Lasso")
matplot(t(glmnetfit$beta[1:10, ]), type = "l", xlab = "Lambda Index",
ylab = "Estimated Beta", main = "Glmnet")
library(glmnet)
glmnetfit = glmnet(X, y, intercept = FALSE)
lambda_all = glmnetfit$lambda
myLasso_pw <- function(X, y, lambda_all, tol = 1e-7, maxitr = 100){
n = nrow(X)
p = ncol(X)
size_lambda = length(lambda_all)
lambda_all = sort(lambda_all, decreasing = TRUE)
beta_mat = matrix(NA, p, size_lambda)
fist_beta = rep(0, p)
beta_new = fist_beta
for (l in 1:size_lambda){
lambda = lambda_all[l]
# the grand loop
for (k in 1:maxitr){
fist_beta = beta_new
r = y - X %*% fist_beta
for (j in 1:p){
r = r + X[, j] * beta_new[j]
# then, we apply the soft threshold
beta_new[j] <- soft_th( X[, j] %*% r / n, lambda)
r = r - X[, j] * beta_new[j]
}
if (sum(abs(beta_new - fist_beta)) < tol) break;
}
beta_mat[, l] = beta_new
}
return(beta_mat)
}
# fit myLasso models
myfit = myLasso_pw(X, y, lambda_all)
#Plot the models
par(mfrow = c(1, 2))
matplot(t(myfit[1:10, ]), type = "l", xlab = "Lambda Index",
ylab = "Estimated Beta", main = "My Lasso")
matplot(t(glmnetfit$beta[1:10, ]), type = "l", xlab = "Lambda Index",
ylab = "Estimated Beta", main = "Glmnet")
library(glmnet)
glmnetfit = glmnet(X, y, intercept = FALSE)
lambda_all = glmnetfit$lambda
myLasso_pw <- function(X, y, lambda_all, tol = 1e-7, maxitr = 100){
n = nrow(X)
p = ncol(X)
size_lambda = length(lambda_all)
lambda_all = sort(lambda_all, decreasing = TRUE)
beta_mat = matrix(NA, p, size_lambda)
fist_beta = rep(0, p)
second_beta = fist_beta
for (l in 1:size_lambda){
lambda = lambda_all[l]
# the grand loop
for (k in 1:maxitr){
fist_beta = second_beta
r = y - X %*% fist_beta
for (j in 1:p){
r = r + X[, j] * second_beta[j]
# then, we apply the soft threshold
second_beta[j] <- soft_th( X[, j] %*% r / n, lambda)
r = r - X[, j] * second_beta[j]
}
if (sum(abs(second_beta - fist_beta)) < tol) break;
}
beta_mat[, l] = second_beta
}
return(beta_mat)
}
# fit myLasso models
myfit = myLasso_pw(X, y, lambda_all)
#Plot the models
par(mfrow = c(1, 2))
matplot(t(myfit[1:10, ]), type = "l", xlab = "Lambda Index",
ylab = "Estimated Beta", main = "My Lasso")
matplot(t(glmnetfit$beta[1:10, ]), type = "l", xlab = "Lambda Index",
ylab = "Estimated Beta", main = "Glmnet")
library(glmnet)
glmnetfit = glmnet(X, y, intercept = FALSE)
lambda_all = glmnetfit$lambda
myLasso_path-wise <- function(X, y, lambda_all, tol = 1e-7, maxitr = 100){
n = nrow(X)
p = ncol(X)
size_lambda = length(lambda_all)
lambda_all = sort(lambda_all, decreasing = TRUE)
beta_mat = matrix(NA, p, size_lambda)
fist_beta = rep(0, p)
second_beta = fist_beta
for (l in 1:size_lambda){
lambda = lambda_all[l]
# the grand loop
for (k in 1:maxitr){
fist_beta = second_beta
r = y - X %*% fist_beta
for (j in 1:p){
r = r + X[, j] * second_beta[j]
# then, we apply the soft threshold
second_beta[j] <- soft_th( X[, j] %*% r / n, lambda)
r = r - X[, j] * second_beta[j]
}
if (sum(abs(second_beta - fist_beta)) < tol) break;
}
beta_mat[, l] = second_beta
}
return(beta_mat)
}
library(glmnet)
glmnetfit = glmnet(X, y, intercept = FALSE)
lambda_all = glmnetfit$lambda
myLasso_pathW <- function(X, y, lambda_all, tol = 1e-7, maxitr = 100){
n = nrow(X)
p = ncol(X)
size_lambda = length(lambda_all)
lambda_all = sort(lambda_all, decreasing = TRUE)
beta_mat = matrix(NA, p, size_lambda)
fist_beta = rep(0, p)
second_beta = fist_beta
for (l in 1:size_lambda){
lambda = lambda_all[l]
for (k in 1:maxitr){
fist_beta = second_beta
r = y - X %*% fist_beta
for (j in 1:p){
r = r + X[, j] * second_beta[j]
second_beta[j] <- soft_th( X[, j] %*% r / n, lambda)
r = r - X[, j] * second_beta[j]
}
if (sum(abs(second_beta - fist_beta)) < tol) break;
}
beta_mat[, l] = second_beta
}
return(beta_mat)
}
# fit myLasso models
myfit = myLasso_pathW(X, y, lambda_all)
#Plot the models
par(mfrow = c(1, 2))
matplot(t(myfit[1:10, ]), type = "l", xlab = "Lambda Index",
ylab = "Estimated Beta", main = "My Lasso")
matplot(t(glmnetfit$beta[1:10, ]), type = "l", xlab = "Lambda Index",
ylab = "Estimated Beta", main = "Glmnet")
library(glmnet)
glmnetfit = glmnet(X, y, intercept = FALSE)
lambda_all = glmnetfit$lambda
myLasso_pathW <- function(X, y, lambda_all, tol = 1e-7, maxitr = 100){
n = nrow(X)
p = ncol(X)
size_lambda = length(lambda_all)
lambda_all = sort(lambda_all, decreasing = TRUE)
beta_mat = matrix(NA, p, size_lambda)
fist_beta = rep(0, p)
second_beta = fist_beta
for (l in 1:size_lambda){
lambda = lambda_all[l]
for (k in 1:maxitr){
fist_beta = second_beta
r = y - X %*% fist_beta
for (j in 1:p){
r = r + X[, j] * second_beta[j]
second_beta[j] <- soft_th( X[, j] %*% r / n, lambda)
r = r - X[, j] * second_beta[j]
}
if (sum(abs(second_beta - fist_beta)) < tol) break;
}
beta_mat[, l] = second_beta
}
return(beta_mat)
}
# fit myLasso models
myfit = myLasso_pathW(X, y, lambda_all)
#Plot the models
par(mfrow = c(1, 2))
matplot(t(myfit[1:10, ]), type = "l", xlab = "Value Lambda",
ylab = "Estimated Beta coefficient", main = "My Lasso")
matplot(t(glmnetfit$beta[1:10, ]), type = "l", xlab = "Value of Lambda",
ylab = "Estimated Beta coefficient", main = "Glmnet")
plot(lambda_all, colSums(abs(beta.diff)), ylab = "L1 norm, 1:10 coefficien
ts: | beta - beta_glm | ")
beta.diff <- myfit - as.matrix(glmnetfit$beta)
max(colSums(abs(beta.diff)))
plot(lambda_all, colSums(abs(beta.diff)), ylab = "L1 norm, 1:10 coefficien
ts: | beta - beta_glm | ")
beta.diff <- myfit - as.matrix(glmnetfit$beta)
max(colSums(abs(beta.diff)))
plot(lambda_all, colSums(abs(beta.diff)), ylab = "L1 norm, 1:100 coefficien
ts: | beta - beta_glm | ")
beta.diff <- myfit[1:10, ] - as.matrix(glmnetfit$beta[1:100, ])
beta.diff <- myfit[1:100, ] - as.matrix(glmnetfit$beta[1:100, ])
max(abs(beta.diff))
plot(lambda_all, colSums(abs(beta.diff)), ylab = "L1 norm, 1:100 coefficien
ts: | beta - beta_glm | ")
blogdown::build_site()
blogdown::serve_site()
knitr::opts_chunk$set(echo = TRUE)
beta.diff <- myfit[1:100, ] - as.matrix(glmnetfit$beta[1:100, ])
library(MASS)
set.seed(10)
n = 100
p = 200
# generate data
V = matrix(0.3, p, p)
diag(V) = 1
X_org = as.matrix(mvrnorm(n, mu = rep(0, p), Sigma = V))
true_b = c(runif(10, -1, 1), rep(0, p-10))
y_org = X_org %*% true_b + rnorm(n)
# pre-scale and center X and y
X = scale(X_org)*sqrt(n/(n-1))
y = scale(y_org)*sqrt(n/(n-1))
lambda = 0.3
#Write myLasso function
soft_th <- function(b, lambda){
return(sign(b) * max(0, abs(b) - lambda))
}
myLasso <- function(X, y, lambda, tol = 1e-7, maxitr = 100){
n = nrow(X)
p = ncol(X)
first_beta = rep(0, p)
second_beta = first_beta
for (k in 1:maxitr){
first_beta = second_beta
r = y - X %*% first_beta
for (j in 1:p){
r = r + X[, j] * second_beta[j]
second_beta[j] <- soft_th( X[, j] %*% r / n, lambda)
r = r - X[, j] * second_beta[j]          }
if (sum((second_beta - first_beta)^2) < tol) break;
}
return(list("N_iteration" = k, "Coef_beta" = second_beta))
}
# fit myLasso model
myfit = myLasso(X, y, lambda = 0.3)
# Determine the number of iterations
myfit$N_iteration
# Find the parameter estimates
myfit$Coef_beta[1:10]
library(glmnet)
glmnetfit = glmnet(X, y, intercept = FALSE)
lambda_all = glmnetfit$lambda
myLasso_pathW <- function(X, y, lambda_all, tol = 1e-7, maxitr = 100){
n = nrow(X)
p = ncol(X)
size_lambda = length(lambda_all)
lambda_all = sort(lambda_all, decreasing = TRUE)
beta_mat = matrix(NA, p, size_lambda)
fist_beta = rep(0, p)
second_beta = fist_beta
for (l in 1:size_lambda){
lambda = lambda_all[l]
for (k in 1:maxitr){
fist_beta = second_beta
r = y - X %*% fist_beta
for (j in 1:p){
r = r + X[, j] * second_beta[j]
second_beta[j] <- soft_th( X[, j] %*% r / n, lambda)
r = r - X[, j] * second_beta[j]
}
if (sum(abs(second_beta - fist_beta)) < tol) break;
}
beta_mat[, l] = second_beta
}
return(beta_mat)
}
# fit myLasso models
myfit = myLasso_pathW(X, y, lambda_all)
#Plot the models
par(mfrow = c(1, 2))
matplot(t(myfit[1:10, ]), type = "l", xlab = "Value of Lambda",
ylab = "Estimated Beta Coefficient", main = "My Lasso")
matplot(t(glmnetfit$beta[1:10, ]), type = "l", xlab = "Value of Lambda",
ylab = "Estimated Beta Coefficient", main = "Glmnet")
beta.diff <- myfit[1:100, ] - as.matrix(glmnetfit$beta[1:100, ])
max(abs(beta.diff))
plot(lambda_all, colSums(abs(beta.diff)), ylab = "Absolute Difference in betas")
beta.diff <- myfit[1:100, ] - as.matrix(glmnetfit$beta[1:100, ])
max(abs(beta.diff))
plot(lambda_all, colSums(abs(beta.diff)), ylab = "Absolute Difference in betas",
xlab = "Value of lambda")
glmnetfit2 = glmnet(X_org, y_org, lambda = lambda_all*sd(y_org)*sqrt(n
/(n-1)))
lassobeta2 = coef(glmnetfit2)[2:11, ]
lassobeta2.all = coef(glmnetfit2)
b = sweep(myfit, 1, apply(X_org, 2, sd), FUN = "/")*sd(y_org)
beta0 = mean(y_org) - colSums(sweep(myfit, 1, apply(X_org, 2, sd) /
apply(X_org, 2, mean),FUN = "/")*sd(y_org))
par(mfrow = c(1, 2))
matplot(t(as.matrix(coef(glmnetfit2)[2:11, ])), type = "l", xlab = "Lamb
da Index",
ylab = "Estimated Beta", main = "Glmnet")
matplot(t(b[1:10, ]), type = "l", xlab = "Lambda Index", ylab = "Estimat
ed Beta",
main = "My Lasso: recovered estimator")
glmnetfit2 = glmnet(X_org, y_org, lambda = lambda_all*sd(y_org)*sqrt(n/(n-1)))
lassobeta2 = coef(glmnetfit2)[2:11, ]
lassobeta2.all = coef(glmnetfit2)
b = sweep(myfit, 1, apply(X_org, 2, sd), FUN = "/")*sd(y_org)
beta0 = mean(y_org) - colSums(sweep(myfit, 1, apply(X_org, 2, sd) /
apply(X_org, 2, mean),FUN = "/")*sd(y_org))
par(mfrow = c(1, 2))
matplot(t(b[1:10, ]), type = "l", xlab = "Value of Lambda", ylab = "Beta Coefficient",
main = "Recovered orinal scale of Beta")
matplot(t(as.matrix(coef(glmnetfit2)[2:11, ])), type = "l", xlab = "Value of Lambda",
ylab = "Beta Coefficient", main = "Glmnet")
max(abs(b[1:10, ] - lassobeta2))
glmnetfit3 = glmnet(X_org, y_org)
max_lambda = max(t(X) %*% y_org) / n
print(max_lambda)
blogdown::build_site()
blogdown::serve_site()
blogdown::serve_site()
setwd("~/Web-Leon/Project1")
blogdown::build_site()
blogdown::build_site()
blogdown::serve_site()
blogdown::build_site()
blogdown::check_site()
blogdown::serve_site()
blogdown::hugo_version()
blogdown::build_site()
blogdown::serve_site()
blogdown::hugo_version()
blogdown::build_site()
blogdown::serve_site()
library(usethis)
use_git_config()
blogdown::build_site()
blogdown::serve_site()
